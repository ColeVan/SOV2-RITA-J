{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c391364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "import time\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import _thread\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5510d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(part, whole):\n",
    "    return part/whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bcd38f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to <Elasticsearch(['https://192.168.1.181:9200'])>: {'name': 'voodoo-onion', 'cluster_name': 'voodoo-onion', 'cluster_uuid': 'dP1VycCSTQ2EPeCVxQ25GA', 'version': {'number': '7.16.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2b937c44140b6559905130a8650c64dbd0879cfb', 'build_date': '2021-12-18T19:42:46.604893745Z', 'build_snapshot': False, 'lucene_version': '8.10.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "def cluster(html_file, connection_id, connection_count, f, MINIMUM_POINTS_IN_CLUSTER, MINIMUM_LIKELIHOOD):\n",
    "    # helpful for debugging - just use a couple of records\n",
    "    # if f.sip.unique()[0] != \"192.168.30.152\" or f.dip.unique()[0] != \"72.52.217.73\":\n",
    "    #     continue\n",
    "\n",
    "    should_write = False\n",
    "\n",
    "    print(f\"Evaluation #{connection_count} ID: {connection_id}: {len(f)} original records...\", end=\"\\r\")\n",
    "    if len(f) <= 4:  # need at least n delta records to make it interesting\n",
    "        return\n",
    "\n",
    "    html = f\"<div id='{connection_id}' class='connection'>\\n\"\n",
    "    html += f.to_html(border=0, classes=\"table table-striped table-sm\").replace(\"<tr>\", \"<tr style='text-align: right'>\")\n",
    "\n",
    "    X = f.iloc[:, [1, 7]]  # X = [connection_id, delta time in milliseconds]\n",
    "\n",
    "    # here we DBSCAN cluster over several different epsilon values calculated from time_spans\n",
    "    # we picked arbitrary spans of time in minutes, but they have served us well in the results\n",
    "    spans = [[0, 5], [2, 15], [15, 35], [30, 60]]\n",
    "\n",
    "    span_count = 0\n",
    "    for span in spans:\n",
    "        span_count += 1\n",
    "        print(f\"Evaluating {connection_id} span {span}...\", end=\"\\r\")\n",
    "        eps = ((span[1] - span[0]) / 2) * .10  # this is our eps calculation: take a span's difference, and halve it, then multiply by .10\n",
    "\n",
    "        t_time = time.time()\n",
    "        # run DBSCAN clustering model...\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=MINIMUM_POINTS_IN_CLUSTER).fit(X)\n",
    "        core_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "        core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "\n",
    "        (unique, counts) = np.unique(dbscan.labels_, return_counts=True)\n",
    "        frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "        dbscan_cluster_data = pd.DataFrame()\n",
    "        dbscan_cluster_data['delta'] = f[\"delta\"]\n",
    "        dbscan_cluster_data['cluster'] = dbscan.labels_\n",
    "        dbscan_cluster_data = dbscan_cluster_data.groupby([\"cluster\"])[\"cluster\"].count().reset_index(name=\"count\")\n",
    "\n",
    "        # calculate likelihood of each cluster\n",
    "        likelihood = percentage(dbscan_cluster_data['count'], dbscan_cluster_data['count'].sum())\n",
    "        # // TODO: need to expand upon likelihood calculation\n",
    "        # could we add this number of items in the cluster vs the largest cluster overall?\n",
    "        # vs the largest cluster in this time span?\n",
    "        dbscan_cluster_data[\"likelihood\"] = likelihood\n",
    "\n",
    "        cols = [col for col in dbscan_cluster_data.columns if col == 'likelihood']\n",
    "        dbscan_cluster_data.loc[dbscan_cluster_data['cluster'] == -1, cols] = 0\n",
    "\n",
    "        for index, row in dbscan_cluster_data.iterrows():\n",
    "            print(f\"Iterating row {index}\", end=\"\\r\")\n",
    "            if row['cluster'] > -1:\n",
    "                if row[\"likelihood\"] > MINIMUM_LIKELIHOOD:\n",
    "                    TOP_TARGETS.extend([[f[\"connection_id\"].iloc[0], f[\"sip\"].iloc[0], f[\"dip\"].iloc[0], row[\"likelihood\"]]])\n",
    "                    TOP_TARGET_DIPS.extend([f[\"dip\"].iloc[0]])\n",
    "                    should_write = True\n",
    "\n",
    "        print(f\"Evaluated {connection_id} span {span} in {(time.time()-t_time)} seconds\", end=\"\\r\")\n",
    "\n",
    "        t_time = time.time()\n",
    "        # get cluster points\n",
    "        unique_labels = set(dbscan.labels_)\n",
    "        for k in unique_labels:\n",
    "            print(f\"Capturing points in and out of cluster {k}\", end=\"\\r\")\n",
    "\n",
    "            class_member_mask = (dbscan.labels_ == k)\n",
    "\n",
    "            # points in cluster\n",
    "            xy = X[class_member_mask & core_samples_mask].to_numpy()\n",
    "            if len(xy[:, 0]) > 0 or len(xy[:, 1]) > 0:\n",
    "                plt.scatter(xy[:, 0] + span_count, xy[:, 1], label=f\"dbscan cluster {eps}\")\n",
    "\n",
    "            # points not in cluster\n",
    "            xy = X[class_member_mask & ~core_samples_mask].to_numpy()\n",
    "            if len(xy[:, 0]) > 0 or len(xy[:, 1]) > 0:\n",
    "                plt.plot(xy[:, 0] + span_count, xy[:, 1], 'x', label=f\"not in dbscan cluster {eps}\")\n",
    "\n",
    "        print(f\"Captured points in and out of cluster {k} in {(time.time()-t_time)} seconds\", end=\"\\r\")\n",
    "\n",
    "        html += f\"<h3>span {span[0]}-{span[1]} minutes. EPS: {eps} Minimum Samples: {MINIMUM_POINTS_IN_CLUSTER}</h3>\\n\"\n",
    "        html += \"<div class='col-4'>\\n\"\n",
    "        html += dbscan_cluster_data.to_html(border=0, classes=\"table table-striped table-sm\").replace(\"<tr>\", \"<tr style='text-align: right'>\")\n",
    "        html += \"</div>\\n\"\n",
    "\n",
    "    plt.grid()\n",
    "    # plot original values\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=\"green\", label=\"original values\")\n",
    "    plt.title('Cluster evaluation for: %d' % connection_id)\n",
    "    plt.legend(loc=\"best\")\n",
    "    # save plot\n",
    "    plt.savefig(f\"data/reports/i/{connection_id}-x.png\")\n",
    "\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    # add plot to html\n",
    "    html += f\"<img src='i/{connection_id}-x.png'>\\n\"\n",
    "    html += \"<div class='topper'>\\n<a href='#top_talkers'>Back to top</a>\\n</div>\\n\"\n",
    "    html += \"</div>\\n\"\n",
    "\n",
    "    if should_write:\n",
    "        with open(html_file, \"a\") as fi:\n",
    "            fi.write(html)\n",
    "\n",
    "\n",
    "plt.switch_backend('Agg')\n",
    "\n",
    "Path(\"data/chunks\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/deltas\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/in\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/reports/i\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "es = Elasticsearch(['https://192.168.1.181:9200'], timeout=30, max_retries=10, retry_on_timeout=True,\n",
    "ca_certs=False,verify_certs=False, http_auth=('jupyter','password'))\n",
    "\n",
    "system_query_table_name = \"query_since_ts\"\n",
    "sleep_time = 60\n",
    "\n",
    "db = sqlite3.connect('app.db')\n",
    "db.row_factory = sqlite3.Row\n",
    "db.execute(f\"create table if not exists {system_query_table_name} (id integer primary key autoincrement, data_source text, ts integer, created timestamp default current_timestamp)\")\n",
    "db.execute(\"create table if not exists deltas (id integer primary key autoincrement, connection_id integer, sip text, dip text, port integer, protocol text, datetime timestamp, delta float)\")\n",
    "\n",
    "# server info?\n",
    "print(f\"Connected to {es}: {es.info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eb0d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchContext = Search(using=es, index='*:so-*', doc_type='doc')\n",
    "index='*:so-*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7fb76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all records since 1970-01-01 00:00:00 (0)\n"
     ]
    }
   ],
   "source": [
    "# get last date queried\n",
    "query_since_ts = 0\n",
    "cursor = db.execute(f\"SELECT ts from {system_query_table_name} WHERE data_source='{index}' order by id desc limit 1\")\n",
    "for row in cursor:\n",
    "    query_since_ts = row[0]\n",
    "    break\n",
    "print(f\"Getting all records since {pd.to_datetime(query_since_ts, unit='s')} ({query_since_ts})\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07c77caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received 10000 records\n",
      "2021-12-30T14:25:22.929Z\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>source.ip</th>\n",
       "      <th>destination.ip</th>\n",
       "      <th>destination.port</th>\n",
       "      <th>network.protocol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-29T23:58:55.889Z</td>\n",
       "      <td>192.168.1.220</td>\n",
       "      <td>224.0.0.252</td>\n",
       "      <td>5355</td>\n",
       "      <td>dns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-29T23:59:00.463Z</td>\n",
       "      <td>192.168.1.220</td>\n",
       "      <td>224.0.0.252</td>\n",
       "      <td>5355</td>\n",
       "      <td>dns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-29T23:59:05.283Z</td>\n",
       "      <td>192.168.128.22</td>\n",
       "      <td>192.168.128.25</td>\n",
       "      <td>88</td>\n",
       "      <td>krb_tcp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-29T23:59:22.053Z</td>\n",
       "      <td>192.168.1.220</td>\n",
       "      <td>224.0.0.252</td>\n",
       "      <td>5355</td>\n",
       "      <td>dns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-29T23:59:26.848Z</td>\n",
       "      <td>192.168.1.220</td>\n",
       "      <td>224.0.0.252</td>\n",
       "      <td>5355</td>\n",
       "      <td>dns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2021-12-30T09:00:10.832Z</td>\n",
       "      <td>192.168.1.154</td>\n",
       "      <td>52.167.249.196</td>\n",
       "      <td>443</td>\n",
       "      <td>ssl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2021-12-30T09:00:10.800Z</td>\n",
       "      <td>192.168.128.25</td>\n",
       "      <td>13.107.222.240</td>\n",
       "      <td>53</td>\n",
       "      <td>dns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2021-12-30T08:59:56.595Z</td>\n",
       "      <td>192.168.1.154</td>\n",
       "      <td>216.239.32.10</td>\n",
       "      <td>53</td>\n",
       "      <td>dns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2021-12-30T08:59:46.760Z</td>\n",
       "      <td>192.168.1.154</td>\n",
       "      <td>65.55.44.109</td>\n",
       "      <td>443</td>\n",
       "      <td>ssl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2021-12-30T08:59:43.089Z</td>\n",
       "      <td>192.168.1.220</td>\n",
       "      <td>224.0.0.252</td>\n",
       "      <td>5355</td>\n",
       "      <td>dns</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ts       source.ip  destination.ip  \\\n",
       "0     2021-12-29T23:58:55.889Z   192.168.1.220     224.0.0.252   \n",
       "1     2021-12-29T23:59:00.463Z   192.168.1.220     224.0.0.252   \n",
       "2     2021-12-29T23:59:05.283Z  192.168.128.22  192.168.128.25   \n",
       "3     2021-12-29T23:59:22.053Z   192.168.1.220     224.0.0.252   \n",
       "4     2021-12-29T23:59:26.848Z   192.168.1.220     224.0.0.252   \n",
       "...                        ...             ...             ...   \n",
       "9995  2021-12-30T09:00:10.832Z   192.168.1.154  52.167.249.196   \n",
       "9996  2021-12-30T09:00:10.800Z  192.168.128.25  13.107.222.240   \n",
       "9997  2021-12-30T08:59:56.595Z   192.168.1.154   216.239.32.10   \n",
       "9998  2021-12-30T08:59:46.760Z   192.168.1.154    65.55.44.109   \n",
       "9999  2021-12-30T08:59:43.089Z   192.168.1.220     224.0.0.252   \n",
       "\n",
       "     destination.port network.protocol  \n",
       "0                5355              dns  \n",
       "1                5355              dns  \n",
       "2                  88          krb_tcp  \n",
       "3                5355              dns  \n",
       "4                5355              dns  \n",
       "...               ...              ...  \n",
       "9995              443              ssl  \n",
       "9996               53              dns  \n",
       "9997               53              dns  \n",
       "9998              443              ssl  \n",
       "9999             5355              dns  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   ts                10000 non-null  object\n",
      " 1   source.ip         10000 non-null  object\n",
      " 2   destination.ip    10000 non-null  object\n",
      " 3   destination.port  10000 non-null  object\n",
      " 4   network.protocol  10000 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "############################################################\n",
    "# get new records from elasticsearch\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "# get last date queried\n",
    "query_since_ts = 0\n",
    "cursor = db.execute(f\"SELECT ts from {system_query_table_name} WHERE data_source='{index}' order by id desc limit 1\")\n",
    "for row in cursor:\n",
    "    query_since_ts = row[0]\n",
    "    break\n",
    "    \n",
    "    \n",
    "# query elasticsearch\n",
    "# '{\"query\":{\"match_all\":{}},\"sort\":[{\"ts\":{\"order\":\"desc\"}}]}'\n",
    "# order by ts asc\n",
    "body_filter = '{\"query\":{\"bool\":{\"filter\":[{\"range\":{@timestamp\":{\"gt\":' + str(query_since_ts) + '}}}]}},\"sort\":[{\"@timstamp\":{\"order\":\"asc\"}}]}'\n",
    "#s = searchContext.query('query_string', query='destination.port:*')\n",
    "s = searchContext.query('query_string', query='network.protocol:*')\n",
    "res = es.search(index=index, size=10000)\n",
    "print(f\"Received {res['hits']['total']['value']} records\")\n",
    "for hit in res['hits']['hits']:\n",
    "    print(hit[\"_source\"][\"@timestamp\"], end=\"\\r\")\n",
    "response = s.execute()\n",
    "df = pd.DataFrame(columns=['ts','source.ip','destination.ip','destination.port', 'network.protocol'])\n",
    "if response.success():\n",
    "    for d in s[:10000]:\n",
    "        try:\n",
    "            df = df.append({'ts' : d['@timestamp'], 'source.ip' : d['source']['ip'], 'destination.ip' : d['destination']['ip'], 'destination.port': d['destination']['port'], 'network.protocol' : d['network']['protocol']}, ignore_index=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "display(df)\n",
    "\n",
    "df.info()\n",
    "\n",
    "src = 'source.ip'\n",
    "dst = 'destination.ip'\n",
    "prt = 'destination.port'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62878f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# filter records that do not cluster or that are irrelevant\n",
    "############################################################\n",
    "if src.startswith(\"192.168.28.25\") or src.startswith(\"192.168.28.15\") or dst.startswith(\"192.168.28.25\") or dst.startswith(\"192.168.28.15\"):  # exclude traffic to and from DCs\n",
    "    is_valid_record = False\n",
    "if src.startswith(\"10.\") or dst.startswith(\"10.\"):  # filter internal traffic\n",
    "    is_valid_record = False\n",
    "if src.startswith(\"7.7.7.\") and dst.startswith(\"7.7.7.\"):  # filter internal traffic\n",
    "    is_valid_record = False\n",
    "if src.startswith(\"192.168.\") and (dst.startswith(\"10.\") or dst.startswith(\"7.7.7.\")):  # filter internal traffic\n",
    "    is_valid_record = False\n",
    "if prt == \"icmp\":\n",
    "    is_valid_record = False\n",
    "\n",
    "db.execute(\n",
    "    f\"INSERT INTO deltas (sip, dip, port, protocol, datetime) values ('source.ip', 'destination.ip', 'destination.port', 'network.protocol', '{pd.to_datetime(df['ts'],infer_datetime_format=True)}')\")\n",
    "query_since_ts = 'ts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0870008",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such column: ts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_439997/2358212089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"INSERT INTO {system_query_table_name} (data_source,ts) values ('{index}', {query_since_ts})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# first we create an empty array to store the delta results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such column: ts"
     ]
    }
   ],
   "source": [
    "db.execute(f\"INSERT INTO {system_query_table_name} (data_source,ts) values ('{index}', {query_since_ts})\")\n",
    "db.commit()\n",
    "\n",
    "# first we create an empty array to store the delta results\n",
    "processed = []\n",
    "last_row = None\n",
    "connection_id = 0\n",
    "\n",
    "############################################################\n",
    "############################################################\n",
    "# calculate each connection's delta time\n",
    "############################################################\n",
    "############################################################\n",
    "cursor = db.execute(f\"SELECT id, sip, dip, port, protocol, datetime from deltas where delta is null order by dip, sip, port, protocol, datetime\")\n",
    "print(f\"Processing {cursor.rowcount} deltas...\")\n",
    "for row in cursor:\n",
    "\n",
    "    if (last_row is not None) and (row[\"sip\"] == last_row[\"sip\"] and (row[\"dip\"] == last_row[\"dip\"]) and (row[\"port\"] == last_row[\"port\"]) and (row[\"protocol\"] == last_row[\"protocol\"])):\n",
    "        # if the new row matches the last row, we want to calculate its delta, so no action needed\n",
    "        pass\n",
    "    else:\n",
    "        # this is a new connection! so we need to remove the last row and increase the delta count\n",
    "        last_row = None\n",
    "        connection_id += 1\n",
    "\n",
    "    if last_row is None:\n",
    "        delta = 0\n",
    "    else:\n",
    "        print(row[\"datetime\"], last_row[\"datetime\"], end=\"\\r\")\n",
    "        # Compute the milliseconds in a timedelta as floating-point number between this row and the previous row\n",
    "        delta = (pd.to_datetime(row[\"datetime\"]) - pd.to_datetime(last_row[\"datetime\"])).total_seconds() * 1e3\n",
    "\n",
    "    db.execute(f\"update deltas set connection_id={connection_id}, delta={delta} where id={row['id']}\")\n",
    "\n",
    "    last_row = row\n",
    "\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668059d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "            ############################################################\n",
    "            ############################################################\n",
    "            # cluster the deltas\n",
    "            ############################################################\n",
    "            ############################################################\n",
    "            TOP_TARGETS = []\n",
    "            TOP_TARGET_DIPS = []\n",
    "            MINIMUM_DELTA = 60000  # this is the minimum delta we will accept write to the delta file (short deltas are not useful and get found out by a \"top talkers\" report)\n",
    "\n",
    "            ds = pd.read_sql_query(\"SELECT * FROM deltas\", db)\n",
    "            connection_ids = ds.connection_id.unique()\n",
    "            print(f\"{len(connection_ids)} unique connection_ids found ({(time.time()-start_time)} seconds)\")\n",
    "            ds[\"delta\"] = ds[\"delta\"] / 1000 / 60\n",
    "\n",
    "            MINIMUM_POINTS_IN_CLUSTER = 4  # this is the minimum number of points we will accept in a cluster, TODO future work to dig deeper into optimum values here\n",
    "            MINIMUM_LIKELIHOOD = .70\n",
    "\n",
    "            id = time.strftime('%Y%m%d-%H%M%S')\n",
    "            start_time = time.time()\n",
    "            html_file = f\"data/reports/report-{id}.html\"\n",
    "            connection_count = 0\n",
    "\n",
    "            with open(html_file, \"w\") as fi:\n",
    "                fi.write(\"\")\n",
    "\n",
    "            threads = []\n",
    "            for connection_id in connection_ids:\n",
    "                connection_count += 1\n",
    "\n",
    "                # get all of the records from the delta file with this connection_id\n",
    "                f = ds.query(f\"connection_id == {connection_id}\").copy()\n",
    "\n",
    "                ############################################################\n",
    "                ############################################################\n",
    "                # _thread.start_new_thread(cluster, (html_file, connection_id, connection_count, f, MINIMUM_POINTS_IN_CLUSTER, MINIMUM_LIKELIHOOD))\n",
    "\n",
    "                cluster(html_file, connection_id, connection_count, f, MINIMUM_POINTS_IN_CLUSTER, MINIMUM_LIKELIHOOD)\n",
    "\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "\n",
    "            print(f\"\\n{connection_count} clusters processed in ({(time.time()-start_time)} seconds)\")\n",
    "\n",
    "            TOP_TARGETS = sorted(set(map(tuple, TOP_TARGETS)), key=lambda x: x[3], reverse=True)\n",
    "            TOP_TARGET_DIPS = sorted(list(set(TOP_TARGET_DIPS)), reverse=True)\n",
    "\n",
    "            # write template file\n",
    "            template_data = \"\"\n",
    "            with open(\"templates/report.html\", 'r+') as f:\n",
    "                template_data = f.read()\n",
    "\n",
    "            html = \"\"\n",
    "            if(len(TOP_TARGETS) > 0):\n",
    "                html += \"<div id='top_talkers' class='row'>\\n\"\n",
    "                html += \"<div class='column'>\\n\"\n",
    "                html += \"<h1>Suspect destination IPs by cluster/likelihood</h1>\\n\"\n",
    "                html += \"<ul>\\n\"\n",
    "                for row in TOP_TARGET_DIPS:\n",
    "                    html += f\"<li>{row}</li>\\n\"\n",
    "                html += \"</ul>\\n\"\n",
    "                html += \"</div>\\n\"\n",
    "                html += \"<div class='column'>\\n\"\n",
    "                html += \"<h3>Clusters of Note</h3>\\n\"\n",
    "                html += \"<ul>\\n\"\n",
    "                for row in TOP_TARGETS:\n",
    "                    html += f\"<li><a href='#{row[0]}'>{row[1]} ==> {row[2]} ({round(row[3]*100)}%)</a></li>\\n\"\n",
    "                html += \"</ul>\\n\"\n",
    "                html += \"</div>\\n\"\n",
    "                html += \"</div>\\n\"\n",
    "\n",
    "            # with open(\"data/chunks/summary.txt\", \"r\") as f:\n",
    "            #     raw = f.read().split(\"|\")\n",
    "            #     template_data = template_data.replace(\"{{start}}\", str(datetime.datetime.fromtimestamp(float(raw[0]))))\n",
    "            #     template_data = template_data.replace(\"{{end}}\", str(datetime.datetime.fromtimestamp(float(raw[1]))))\n",
    "            #     template_data = template_data.replace(\"{{file_count}}\", \"{:,d}\".format(int(raw[2])))\n",
    "            #     template_data = template_data.replace(\"{{total_records}}\", \"{:,d}\".format(int(raw[3])))\n",
    "            #     template_data = template_data.replace(\"{{filtered_records}}\", \"{:,d}\".format(int(raw[4])))\n",
    "\n",
    "            with open(html_file, 'r+') as f:\n",
    "                content = f.read()\n",
    "                f.seek(0, 0)\n",
    "\n",
    "                template_data = template_data.replace(\"{{top_talkers}}\", html)\n",
    "                template_data = template_data.replace(\"{{report}}\", content)\n",
    "                template_data = template_data.replace(\"{{id}}\", id)\n",
    "\n",
    "                f.write(template_data)\n",
    "\n",
    "            print(f\"Found {len(TOP_TARGETS)} top targets in {time.time() - start_time}.\")\n",
    "            print(f\"View the report here: {html_file}\")\n",
    "\n",
    "            ############################################################\n",
    "            ############################################################\n",
    "            # time to do it again!\n",
    "            ############################################################\n",
    "            ############################################################\n",
    "            print(f\"Sleeping for {sleep_time} seconds...\", end=\"\\r\")\n",
    "            time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbed26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
